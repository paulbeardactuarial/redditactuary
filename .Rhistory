library(RedditExtractoR)
library(tidyverse)
# data scrape - ActuaryUK
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="ActuaryUK",
period="all")
top.url <- top.actuary.posts %>% .$url
df <- get_thread_content(top.url)
saveRDS(df,"./Data/ActuaryUK.all.threads.rds")
warnings()
saveRDS(df,"./Data/ActuaryUK.all.threads.rds")
str(df)
ActuaryUK.all.threads <- readRDS("C:/Users/paulb/R Projects/redditactuary/Data/Previous/ActuaryUK.all.threads.rds")
df
?get_thread_content
library(RedditExtractoR)
library(tidyverse)
?get_thread_content
# data scrape
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="Actuary",
period="all")
top.url <- top.actuary.posts %>% .$url
library(RedditExtractoR)
library(tidyverse)
top.url <- top.actuary.posts %>% .$url
top.url
df <- get_thread_content(top.url)
df <- get_thread_content(top.url)
write_rds(df,"./Data/Actuary.all.threads.rds")
saveRDS(df,"./Data/ActuaryUK.all.threads.rds")
str(df)
df$threads$url
top.url
top.url %>% split()
split(top.url, ceiling(seq_along(d)/10))
split(top.url, ceiling(seq_along(top.url)/10))
split(top.url, ceiling(seq_along(top.url)/100))
carved.url <- split(top.url, ceiling(seq_along(top.url)/100))
carved.url
df <- map(carved.url, get_thread_content)
df <- map(carved.url, get_thread_content)
carved.url <- split(top.url, ceiling(seq_along(top.url)/100))
df <- map(carved.url, get_thread_content)
top.url <- top.actuary.posts %>% .$url
# data scrape
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="Actuary",
period="all")
top.url <- top.actuary.posts %>% .$url
carved.url <- split(top.url, ceiling(seq_along(top.url)/100))
library(RedditExtractoR)
library(tidyverse)
# data scrape
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="Actuary",
period="all")
top.url <- top.actuary.posts %>% .$url
carved.url <- split(top.url, ceiling(seq_along(top.url)/100))
df <- map(carved.url, get_thread_content)
write_rds(df,"./Data/Actuary.all.threads.rds")
carved.url
get_thread_content(carved.url$`1`)
df1 <- get_thread_content(carved.url$`1`)
str(df1)
df1
Actuary.all.threads <- readRDS("C:/Users/paulb/R Projects/redditactuary/Data/Previous/Actuary.all.threads.rds")
ActuaryUK.all.threads <- readRDS("C:/Users/paulb/R Projects/redditactuary/Data/Previous/ActuaryUK.all.threads.rds")
gc()
gc()
library(RedditExtractoR)
library(tidyverse)
# data scrape - ActuaryUK
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="ActuaryUK",
period="all")
top.url <- top.actuary.posts %>% .$url
df <- get_thread_content(top.url)
saveRDS(df,"./Data/ActuaryUK.all.threads.rds")
str(df)
warnings()
df$threads
df$threads$url
df$comments$url
ActuaryUK.all.threads <- readRDS("C:/Users/paulb/R Projects/redditactuary/Data/Previous/ActuaryUK.all.threads.rds")
closeAllConnections()
ActuaryUK.all.threads <- readRDS("C:/Users/paulb/R Projects/redditactuary/Data/Previous/ActuaryUK.all.threads.rds")
ActuaryUK.all.threads$comments$url
Actuary.all.threads <- readRDS("C:/Users/paulb/R Projects/redditactuary/Data/Previous/Actuary.all.threads.rds")
Actuary.all.threads$comments$url
Actuary.all.threads$comments %>% str()
top.actuary.posts
top.actuary.posts$title
top.actuary.posts[[947]]
top.actuary.posts[[947,]]
top.actuary.posts[947,]
df <- map(carved.url, function(x) safely(get_thread_content))
carved.url <- split(top.url, ceiling(seq_along(top.url)/100))
# data scrape
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="Actuary",
period="all")
top.url <- top.actuary.posts %>% .$url
carved.url <- split(top.url, ceiling(seq_along(top.url)/100))
gc()
# data scrape
top.actuary.posts <- RedditExtractoR::find_thread_urls(
sort_by = "top",
subreddit="Actuary",
period="all")
